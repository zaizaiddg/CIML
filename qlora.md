## LoRA

<img src="https://cdn.jsdelivr.net/gh/steven-lbp/images@main/image-20241027141938004.png" alt="image-20241027141938004" style="zoom: 67%;" />



其中蓝色部分是原始的预训练模型权重记作$W_{0}$。LoRA冻结$W_{0}$， 然后在旁边添加了一个可训练的新矩阵记作$\Delta W$，并且$\Delta W$可分解为两个矩阵$A$和$B$相乘而得， 所以LoRA的模型变成的输出变成了$h = (W_{0}+\Delta W)x$。并且由于$A$矩阵的权重是一个标准正态分布，而$B$矩阵初始化是一个全为0的矩阵，所以最开始的输出还是依旧为$W_{0}x$。





## QLoRA

### 1. 4-bit NormalFloat Quantization

NormalFloat是一种数据类型，它是建立在Quantile quantization（分位数量化：简单来说就是把顺序排列的一组数据分割为若干个相等块的分割点的数值）基础上的，是一种信息论上最优的数据类型，可以确保每个量化区间从输入张量中分配相同数量的值。分位数量化通过经验累积分布函数估计输入张量的分位数来工作。



在神经网络中，预训练的权重通常具有零中心的正态分布，标准差为σ。通过缩放σ，可以使得分布恰好适应NF的范围。对于NF，作者设置了一个任意的范围[-1, 1]。因此，数据类型和神经网络权重的分位数都需要被归一化到这个范围



### 2. Double Quantization（双重量化）

为了降低Outlier的影响，作者采用分块的方式进行进行双重量化。

第一次的量化常数作为第二次量化的输入。第二步产生的量化常数以及第二层产生的量化常数。具体来说就是每64个参数共享一个量化常数(Absmax, 32bit)，这样的话相当于每一个参数的量化额外开销为32 / 64 = 0.5 bit，在几B甚至几十B的大模型上这也是一笔巨大的开销，所以为了进一步优化这个量化开销，作者对量化后的常数采用256的块大小对量化常数进行二次FP8量化，这样可以节省内存至0.127 bit

### 3.Page Optimizer（分页优化器）

梯度检查点是用于解决模型训练时显存占用过高的问题的一个技术方案。我们知道在模型训练时，我们通常需要将所有前向传播的激活值保存下来以在模型进行反向传播的时候使用，但是这样就会非常占用模型显存。当然我们也可以不保存激活值，而是在计算梯度时重新计算，但是这样虽然减少了缓存占用，但是却增大了计算量，减慢了训练速度。

梯度检查点就是一个介于全不丢和全丢弃的一个这种的技术方案。有保存的梯度我们就直接使用这个保存的值， 没有保存的梯度时我们再根据它损失函数重新计算这个梯度

而分页优化器则是针对梯度检查点做的进一步优化，利用NVIDIA统一内存的功能，在GPU偶尔因内存不足(OOM)而出现情况时，将保存的部分梯度检查点转移到CPU内存上，而在优化器更新阶段需要内存时则将其分页回到GPU内存中，实现CPU和GPU之间的自动页面传输，确保GPU处理过程无误。该功能类似于CPU RAM和磁盘之间的常规内存分页操作。



## 模型测试

基于以上三点优化QLoRA取得了非常不错的成绩，作者提出了的

1. QLoRA可以在一个消费级的48GB的显卡上去媲美一个16bit的全量微调的效果，而这个正常来说是需要780GB显存才能全量微调的。

2. 基于QLoRA对OASST1微调出来的Guanaco家族大模型性能如下表，他们在单个消费级别的显卡训练不到12小时在Vicuna基准上可以达到ChatGPT 97.8%的性能水平，如果训练时间达到24小时后，其最大的Guanaco模型可以达到99.3%，并且最小的Guanaco模型仅需要5GB显存。

<img src="https://cdn.jsdelivr.net/gh/steven-lbp/images@main/image-20241027142857726.png" alt="image-20241027142857726" style="zoom:50%;" />

3. 作者还对比了不同的数据类型的影响，如下图，在0样本的任务中可以明显看出NF4相比于float4有更好的提升，除此之外，DQ(双重量化)对性能同样也有些许的提升，主要还是减少内存

<img src="https://cdn.jsdelivr.net/gh/steven-lbp/images@main/image-20241027142941942.png" alt="image-20241027142941942" style="zoom:50%;" />

4. 作者使用4-bitQLoRA量化技术对不同规模参数模型进行调优，并与16位精度调优进行对比。结果表明，使用双量化的NF4可以匹配BF16性能，并且FP4调优结果都要比两者低一个百分点。

![image-20241027143111502](https://cdn.jsdelivr.net/gh/steven-lbp/images@main/image-20241027143111502.png)

## 总结

QLoRA本身并不是一个创新性的工作，而是将多个工作融合在一起，逐一击破以前方法的痛点，LoRA可以达到全量微调的效果，NF4量化和双量化两种策略实现高保真度4位微调，分页优化器可以解决显存峰值显存不足(OOM)的问题。这些方法加起来实现了1+1>2的效果，这才实现了单张消费级显卡微调33B大模型、单张专业级显卡微调65B大模型。文章的前半部分主要是为了证明QLoRA的方法能够在低计算资源场景进行微调，后半部分更多是介绍Guanaco模型，来验证QLoRA的性能

Guanaco是一个基于QLoRA微调的大模型，作者通过定性和定量的分析，和其他大模型进行了对比，证明Guanaco的性能与ChatGPT相当，间接证明了QLoRA在性能上达到了全量微调的能力，此外还对模型的一些其他性质进行了进一步的探索