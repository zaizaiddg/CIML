# QLoRA **4-bit NormalFloat Quantization(NF4)**

## 背景
<img src="https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/LLM3.png">

语言模型一直在变大，由于这些模型很大，因此它们很难在一般的设备上运行。举个例子，仅推理 BLOOM-176B 模型，你就需要 8 个 80GB A100 GPU (每个约 15,000 美元)。而如果要微调 BLOOM-176B 的话，你需要 72 个这样的 GPU！更大的模型，如 PaLM，还需要更多资源。

## 介绍
为了解决这个问题，作者提出了QLoRA这种方法。其中4-bit NormalFloat Quantization（NF4）量化为QLoRA中的核心创新。

## 前置知识
在介绍NF4量化之前，我们先需要理解一下不同的数据类型，这些数据类型在机器学习中被称为精度。模型的大小通常由其参数量及其精度决定，精度通常为 float32、float16 或 bfloat16之一，如下图所示。
<img src="https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/tf32-Mantissa-chart-hi-res-FINAL.png">

在理解NF4量化之前，我们还需要理解量化。对于量化，简而言之就是以较少的位数来存储信息，并使模型仍达到一个不错的效果。

化分为量化和反量化两个阶段。下图展示的是将FP16类型的数据量化为INT4类型的数据，并反量化INT4类型的数据来估计原本对应的数据。  **注意：在反量化之后得出的估计数据与原数据有误差！**

<img src="https://github.com/zaizaiddg/CIML/blob/cc874714181745625442cb4490d44349a59daec5/%E5%9B%BE%E7%89%87/%E9%87%8F%E5%8C%96.png?raw=true">

> 关于此图的详细理解:  
>**量化阶段**：  
>step1:我们需要量化[0,27,0.58,-0.75,1.52]这四个浮点数权重，这四个浮点数是采用float16(FP16)来存储的，因此此时需要的内存容量为16*4=64bit。  
>
>step2:将每个数据除以其中的最大值，此时我们就可以将其标准化至[-1,1]范围之中。  
>
>step3:将[-1,1]平均分为16份，因为我们要将FP16类型的数据量化为INT4类型的数据(4bit整数，表示的范围为[0,15])。平分为16份之后便得到了16个分位点，之后将标准化后的数据匹配其最近的分位点。
>```
>quantiles = [-1., -0.86666667, -0.73333333, -0.6, -0.46666667,-0.33333333, -0.2, -0.06666667, 0.06666667, 0.2,0.33333333, 0.46666667, 0.6, 0.73333333, 0.86666667, 1.]
>```  
>
>step4:匹配此分位点对应的index，此时我们将[0,27,0.58,-0.75,1.52]量化为了[9,10,4,15]。此时所需要的内存容量为4*4+16=32bit(因为我们还要存储max值用来在反量化阶段估计数值所以要加上16)  
>
>**反量化阶段**：  
>step1:根据对应的index找到此分位数  
>
>step2:将此分位数乘以max值之后便得到估计数值

**注意：得到这种分位数结果的前提是假设[-1,1]是均匀的分布，下图为[-1,1]之间均匀分布的概率密度函数。**
<img src="https://github.com/zaizaiddg/CIML/blob/master/%E5%9B%BE%E7%89%87/simplequant-int4.png?raw=true">

## 4-bit NormalFloat Quantization(NF4)

4-bit NormalFloat(NF4)是一种数据类型(类似上文所说的INT4)，它在量化过程中保留了零点，并使用所有$2^k$位来表示$k$位数据类型。这种数据类型通过估计两个范围的分位数$q_i$来创建一个非对称的数据类型，这两个范围分别是负数部分$[-1,0]$的$2^{k-1}$和正数部分$[0,1]$的$2^{k-1}+1$。然后，它统一了这两组分位数$q^i$，并从两组中都出现的两个零中移除一个。这种结果数据类型在每个量化桶中都有相等的期望值数量，因此被称为$\text{k-bit NormalFloat}\space (\text{NF}_k)$，这种数据类型对于以零为中心的正态分布数据在信息论上是最优的。
>预训练的神经网络权重具有以零中心的正态分布的性质（因此NF4数据类型比INT4和Float4更适用于微调神经网络），通过缩放标准差σ，使其权重落在[-1,1]上，分位数也落在这个范围内。

我们使用下面的公式来计算具体的分位数，
$$
\begin{equation}
q_i=\frac{1}{2}\left(Q_X\left(\frac{i}{2^k+1}\right)+Q_X\left(\frac{i+1}{2^k+1}\right)\right)
\end{equation}
$$
$$
\text { where } Q_X(\cdot) \text { is the quantile function of the standard normal distribution } N(0,1)
$$
标准正态分布量化函数把[-1, 0]分成7份，然后生成[-1, ..., 0]共8个分位数, 把[0, 1]分成8份，然后生成[0, ..., 1]共9个分位数，两个合起来去掉一个0就生成全部的16个分位数了。

>算法解释:  
>对于这个函数的简要解释是：它创建了NF4数据类型的16个值，并用零填充，以便在8位量化函数中使用（256个值，其中包括256-16个零）。该函数在bitsandbytes库中使用8位量化方法来“模拟”NF4。尽管算法可能有些晦涩，但以下是更直观的解释：
我们的目标是找到等面积的量化区间，使得量化区间左右两侧的面积相等。这意味着我们不从正态分布的0和1量化区间开始，而是从一个偏移量量化区间开始。代码片段中称之为"offset"，其值为1-1/(215)。如果我们有一个非对称的数据类型，其中一侧的间隔等于每个量化区间周围的16个“半个”，而另一侧只有15个“半个”。因此，平均偏移量为(1-1/(215) + 1-1/(2*16))/2 = 0.9677083。
我们使用norm.ppf函数获取标准正态分布（N(0, 1)）的量化区间。然后，通过将这些量化区间的值除以绝对最大值来重新缩放它们。  

<img src="https://github.com/zaizaiddg/CIML/blob/master/%E5%9B%BE%E7%89%87/nf4.png?raw=true">

<img src="https://github.com/zaizaiddg/CIML/blob/master/%E5%9B%BE%E7%89%87/nf4_2.png?raw=true">

